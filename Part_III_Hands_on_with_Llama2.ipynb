{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HansHenseler/DFRWS-APAC-LLM-Workshop/blob/main/Part_III_Hands_on_with_Llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III: Hands on with Llama2\n",
        "\n",
        "This is an example for Instruction Answering and Chat Conversation. In the instruction answering you give an instruction to the LLM and the LLM makes the responses (Stage 3). In the chat conversation the LLM remembers previous prompts and answers (Stage 4).\n",
        "\n",
        "\n",
        "Model used: llama-2-13b-chat.Q4_0.gguf\n",
        "Model: Llama 2,\n",
        "Size: 13B,\n",
        "Model type: Chat model\n",
        "Quantization: 4bit, native method\n",
        "Model Format: gguf\n",
        "\n",
        "First step: GET A GPU !!!\n"
      ],
      "metadata": {
        "id": "QVECpMJqLvGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1: Then install langchain and llama-cpp-python"
      ],
      "metadata": {
        "id": "npMLUhM-Lnj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG1tp3oxL4qg",
        "outputId": "8f36c87a-8a64-458a-cd63-0f8343499c1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.314-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.314 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Llamma cpp python"
      ],
      "metadata": {
        "id": "ZO6hgFLsLqbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LLAMA_CUBLAS=1\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ4UuiVqMuXz",
        "outputId": "f9818e16-7b54-46ce-9a49-c4913820520d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=6423529 sha256=ab60ddba5ef831fab6c9c772a12d138a8439d43507b1b9d1f70e2c2e1b46b474\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 2: Get the Llama 2 13B chat model"
      ],
      "metadata": {
        "id": "FTxnea1fkNfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the chat model!\n",
        "!wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj7-_-1SP3QQ",
        "outputId": "18767c89-50f7-41e4-9b45-6f4e894ac70a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 04:08:30--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 108.138.94.27, 108.138.94.97, 108.138.94.45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.138.94.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/eda2a15d532bea4ce6fc14d15c2b72638243396816252f73a94dceeb0429112f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q4_0.gguf%3B+filename%3D%22llama-2-13b-chat.Q4_0.gguf%22%3B&Expires=1697688510&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzY4ODUxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VkYTJhMTVkNTMyYmVhNGNlNmZjMTRkMTVjMmI3MjYzODI0MzM5NjgxNjI1MmY3M2E5NGRjZWViMDQyOTExMmY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ZD3uARV9uUxFFrSncucAISy5spwBpI1XnMkd3jNUKJ38q41yC-7%7EJZAFxT0lR2WTXoe0krB5jZqPfT%7EWekiwmcFWGHBgySj3SxERdOYJG6gTPCZtwLyEO4ACoJf26gVBTKyRweitTakCGm36UkJQ2cKSx28qvyrEN6uKfAfDdMSoBln3K1aim8IMc1ckmhsSshqX-f81jR5ijyw%7Eovz4gtaxZB8VqAujXvDarHTyLup8UTatK5jc7Fc%7Ea72ZxDJXLf6KgKMQ%7EjlPTLYpoceCXjQSbRn2OqFmIHj1yiOwZTM9o4F-d50mRaPMkDEIFspAxpGWoDdycCoYYCbNYCe2IA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-10-16 04:08:30--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/eda2a15d532bea4ce6fc14d15c2b72638243396816252f73a94dceeb0429112f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q4_0.gguf%3B+filename%3D%22llama-2-13b-chat.Q4_0.gguf%22%3B&Expires=1697688510&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NzY4ODUxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VkYTJhMTVkNTMyYmVhNGNlNmZjMTRkMTVjMmI3MjYzODI0MzM5NjgxNjI1MmY3M2E5NGRjZWViMDQyOTExMmY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=ZD3uARV9uUxFFrSncucAISy5spwBpI1XnMkd3jNUKJ38q41yC-7%7EJZAFxT0lR2WTXoe0krB5jZqPfT%7EWekiwmcFWGHBgySj3SxERdOYJG6gTPCZtwLyEO4ACoJf26gVBTKyRweitTakCGm36UkJQ2cKSx28qvyrEN6uKfAfDdMSoBln3K1aim8IMc1ckmhsSshqX-f81jR5ijyw%7Eovz4gtaxZB8VqAujXvDarHTyLup8UTatK5jc7Fc%7Ea72ZxDJXLf6KgKMQ%7EjlPTLYpoceCXjQSbRn2OqFmIHj1yiOwZTM9o4F-d50mRaPMkDEIFspAxpGWoDdycCoYYCbNYCe2IA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.14, 108.138.94.122, 108.138.94.23, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7365834624 (6.9G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q4_0.gguf’\n",
            "\n",
            "llama-2-13b-chat.Q4 100%[===================>]   6.86G   235MB/s    in 44s     \n",
            "\n",
            "2023-10-16 04:09:15 (159 MB/s) - ‘llama-2-13b-chat.Q4_0.gguf’ saved [7365834624/7365834624]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-13b-chat.Q4_0.gguf\", # location of the model\n",
        "    temperature=0.75,                 # temperature\n",
        "    max_tokens=2000,                 # Max. number of tokens to be generated\n",
        "    top_p=0.9,                    # top_p = 0.9\n",
        "    top_k=30,                     # top_k = 30\n",
        "    n_gpu_layers=43,                 # number of layers to offload to GPU\n",
        "    verbose=True, # Verbose is required to pass to the callback manager\n",
        "    n_batch=200,          # number of token generation in parallel\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0APaGROMRMC",
        "outputId": "879df5cb-217c-4491-a59d-5b120b7154de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8PNez0eOP08R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39220177-fe85-49f0-b299-0c04963ebc20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 16 04:18:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    27W /  70W |   7949MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 3: Try the model with Instruction Answering"
      ],
      "metadata": {
        "id": "g6oL_Hj9kV3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"what is water?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ScRdcYAQ72XM",
        "outputId": "e66c0ed3-787b-4c50-f593-972aafa9667c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwater is a colorless, odorless, and flavorless liquid that makes up approximately 71% of the earth's surface. it is essential for human survival and is used in many different ways, including drinking, irrigation, energy production, and cleaning.\\n\\nwater is composed of two hydrogen atoms and one oxygen atom, with the chemical formula h2o. it is a polar molecule, meaning that it has a slightly positive charge on the hydrogen atoms and a slightly negative charge on the oxygen atom. this polarity allows water to dissolve a wide variety of substances and to be an effective solvent for many different compounds.\\n\\nwater plays a crucial role in many biological processes, including photosynthesis, respiration, and the regulation of body temperature. it is also an important component of many industrial processes, such as manufacturing, transportation, and energy production.\\n\\n despite its importance, water scarcity is a growing concern in many parts of the world, particularly in regions with high population growth, limited access to clean water, and climate change. this has led to an increased focus on water conservation, wastewater treatment, and desalination technologies.\\n\\noverall, water is a vital resource that is essential for human survival and economic development. it is important to protect and conserve this valuable resource for future generations.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"plan a 2 days trip in Singapore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "j-0xjdaf9kCk",
        "outputId": "11e73ec9-5884-4a3c-90bd-efe677cd66f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' and want to know which places you can visit? Here is the list of top 10 tourist attractions in Singapore that you must visit:\\n1. Gardens by the Bay - a futuristic park featuring large botanical gardens, including the Flower Dome cooled conservatory and the Cloud Forest cloud forest, with elevated walkways for views of the city skyline.\\n2. Marina Bay Sands SkyPark - an observation deck on top of the Marina Bay Sands hotel, offering panoramic views of the city and the Marina Bay area. \\n3. Merlion - a mythical creature that is half-mermaid, half-lion, located near the Marina Bay waterfront and spouts out water into the Singapore River.\\n4. Chinatown - a historic district with colorful street art, temples, markets, and authentic Chinese cuisine.\\n5. Little India - a vibrant neighborhood with colorful street art, temples, markets, and authentic Indian cuisine.\\n6. National Gallery Singapore - an art museum featuring modern and contemporary Southeast Asian art, housed in two historic buildings. \\n7. Clarke Quay - a nightlife district along the Singapore River, lined with bars, clubs, and restaurants. \\n8. Jurong Bird Park - a bird sanctuary with over 500 species of birds from around the world, including penguins, flamingos, and parrots.\\n9. Haw Par Geles - an amusement park featuring a Giant Panda exhibit, a Magical Crystal Cave, and a Mysterious Grove area with waterfalls and gardens.\\n10. ArtScience Museum - a museum dedicated to art and science, featuring interactive exhibitions and installations that explore the relationship between creativity and technology. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = '''\n",
        "1. Gardens by the Bay - I want to see the Supertree Grove and the Flower Dome and Cloud Forest Domes.\n",
        "2. Marina Bay Sands - I want to take a photo with the iconic hotel and enjoy the light and water show at night.\n",
        "3. Merlion - I want to see the famous statue and take a photo with it.\n",
        "4. Singapore Flyer - I want to ride the giant Ferris wheel and enjoy the panoramic view of the city.\n",
        "5. Chinatown - I would like to explore the colorful streets and try some delicious street food.\n",
        "6. Little India - I would like to experience the vibrant Indian culture and try some authentic Indian cuisine.\n",
        "7. Haw Par Villa - I want to see the largest theme park in Singapore and enjoy the Chinese mythology-themed rides and attractions.\n",
        "8. Clarke Quay - I would like to visit the nightlife district and enjoy the bars, clubs, and live music performances.\n",
        "'''\n",
        "llm(\"Please summarize the following passage in 50 words. Passage: \" + txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "PolJagGH-sYt",
        "outputId": "c694439a-6dd4-4dae-a534-095920bedcd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'9. Orchard Road - I want to go shopping at the famous shopping street and explore the various malls and department stores.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"which countries are located in Europe\")"
      ],
      "metadata": {
        "id": "Vo8muDZAr17k",
        "outputId": "ed67f0c5-db86-4450-df3c-6c5a77cff5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"?\\n\\nThere are 50 countries located in Europe, and they are:\\n\\n1. Albania\\n2. Andorra\\n3. Armenia\\n4. Austria\\n5. Azerbaijan\\n6. Belarus\\n7. Belgium\\n8. Bosnia and Herzegovina\\n9. Bulgaria\\n10. Croatia\\n11. Cyprus\\n12. Czech Republic\\n13. Denmark\\n14. Estonia\\n15. Faroe Islands\\n16. Finland\\n17. France\\n18. Georgia\\n19. Germany\\n20. Gibraltar\\n21. Greece\\n22. Hungary\\n23. Iceland\\n24. Ireland\\n25. Italy\\n26. Kazakhstan\\n27. Kosovo\\n28. Latvia\\n29. Liechtenstein\\n30. Lithuania\\n31. Luxembourg\\n32. Malta\\n33. Moldova\\n34. Monaco\\n35. Montenegro\\n36. Netherlands\\n37. Norway\\n38. Poland\\n39. Portugal\\n40. Romania\\n41. Russia (partially)\\n42. San Marino\\n43. Serbia\\n44. Slovakia\\n45. Slovenia\\n46. Spain\\n47. Sweden\\n48. Switzerland\\n49. Turkey (partially)\\n50. Ukraine\\n\\nNote that some countries have territories located outside of Europe, such as Russia's Kaliningrad Oblast and Turkey's Istanbul province. Additionally, some countries have disputed borders or territorial claims, so this list may not be exhaustive or definitive.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 4: Try the model with Chat Conversation\n"
      ],
      "metadata": {
        "id": "IU3vwDtwYfYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "model_path = \"/content/llama-2-13b-chat.Q4_0.gguf\"\n",
        "llm = LlamaCpp(model_path=model_path,\n",
        "               # n_ctx=4096,\n",
        "               seed=0,\n",
        "               temperature=0.1,\n",
        "               top_k=30,  # original 50\n",
        "               top_p=0.75,# original 0.95\n",
        "               n_batch=300,      # num. of parallel generation\n",
        "               n_gpu_layers=43,  # important for GPU\n",
        "               n_threads = 10,   # important for CPU\n",
        "               max_tokens=1024,\n",
        "               repeat_penalty=1.18,\n",
        "               )\n",
        "\n",
        "history = []\n",
        "\n",
        "def convert_message_and_history_to_llama2_chat_format(message: str, memory_limit = 7) -> str:\n",
        "    # memory_limit set to 7 -> only memorize latest 7 pairs of \"user, assistant\" conversation\n",
        "    global history\n",
        "\n",
        "    SYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\n",
        "    You are a helpful bot. Your answers are clear and concise.\n",
        "    <</SYS>>\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if len(history) > memory_limit:\n",
        "        history = history[-memory_limit:]\n",
        "\n",
        "    if len(history) == 0:\n",
        "        return SYSTEM_PROMPT + f\"{message} [/INST]\"\n",
        "\n",
        "    # first message pair is treated differently\n",
        "    llama2_history_formated_message = SYSTEM_PROMPT + f\"user{history[0][0]} [/INST] {history[0][1]} </s>\"\n",
        "    # for 2nd and further message pairs\n",
        "    for user_msg, assistant_msg in history[1:]:\n",
        "        llama2_history_formated_message += f\"<s>[INST] {user_msg} [/INST] {assistant_msg} </s>\"\n",
        "    # for the current user message\n",
        "    llama2_history_formated_message += f\"<s>[INST] {message} [/INST]\"\n",
        "\n",
        "    return llama2_history_formated_message\n",
        "\n",
        "\n",
        "def generate_response(message: str) -> str:\n",
        "    global history\n",
        "\n",
        "    query = convert_message_and_history_to_llama2_chat_format(message)\n",
        "    responses = llm(query)\n",
        "    history.append((message, responses))\n",
        "    print(\"user: \", message)\n",
        "    print(\"assistant: \", responses)\n",
        "    print('\\n')\n"
      ],
      "metadata": {
        "id": "b63RFQhYYk35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\"hi\")\n",
        "generate_response(\"what is iron?\")\n",
        "generate_response(\"what is the boiling point of it?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uW-HEjSbEWJ",
        "outputId": "67775de5-01c2-4fe6-e1d7-215d5817a001"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user:  hi\n",
            "assistant:    Hello! I'm here to help answer any questions you may have. What can I assist you with today?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user:  what is iron?\n",
            "assistant:    Iron is a chemical element with the symbol Fe and atomic number 26. It is a metal that belongs to the group of transition metals and is one of the most common elements on Earth.\n",
            "\n",
            "Iron is an essential nutrient for many living organisms, including humans, where it plays a crucial role in the functioning of red blood cells and the transportation of oxygen throughout the body. It is also a key component in the synthesis of hemoglobin, which is the protein in red blood cells that carries oxygen.\n",
            "\n",
            "In addition to its biological importance, iron is also a critical material in many industrial processes, such as the production of steel, which is an alloy of iron and carbon. Steel is a key component in construction, transportation, and other industries, and it is estimated that over 90% of all metal produced worldwide is steel.\n",
            "\n",
            "Iron is also a significant component in the Earth's core, where it is thought to make up about 85% of the planet's total iron content. It is believed that the Earth's core is composed primarily of iron and nickel, with smaller amounts of lighter elements such as oxygen, silicon, and sulfur.\n",
            "\n",
            "Overall, iron is a versatile element that plays a vital role in many aspects of our lives, from the functioning of living organisms to the production of industrial materials.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user:  what is the boiling point of it?\n",
            "assistant:    The boiling point of iron is approximately 2860°C (5176°F) at standard atmospheric pressure. This means that iron melts at a temperature of around 1538°C (2800°F) and boils at a temperature of around 2860°C (5176°F). However, it's important to note that the exact boiling point of iron can vary depending on the specific conditions in which it is being he\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}